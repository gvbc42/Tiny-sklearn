{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import VotingClassifier as skVotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VotingClassifier():\n",
    "    def __init__(self, estimators, voting='hard'):\n",
    "        self.estimators = estimators\n",
    "        self.voting = voting\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_, y_train = np.unique(y, return_inverse=True)\n",
    "        self.estimators_ = [deepcopy(est).fit(X, y_train) for _, est in self.estimators]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.voting == 'hard':\n",
    "            prob = np.array([est.predict(X) for est in self.estimators_]).T\n",
    "        elif self.voting == 'soft':\n",
    "            prob = np.array([est.predict_proba(X) for est in self.estimators_])\n",
    "        return prob\n",
    "\n",
    "    def predict(self, X):\n",
    "        prob = self.transform(X)\n",
    "        if self.voting == 'hard':\n",
    "            pred = np.apply_along_axis(lambda x:np.argmax(np.bincount(x)), axis=1, arr=prob)\n",
    "        elif self.voting == 'soft':\n",
    "            pred = np.argmax(np.mean(prob, axis=0), axis=1)\n",
    "        return self.classes_[pred]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.voting == 'hard':\n",
    "            raise AttributeError\n",
    "        return np.mean(self.transform(X), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft voting\n",
    "X, y = load_iris(return_X_y = True)\n",
    "clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=15000, random_state=0)\n",
    "clf2 = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)]).fit(X, y)\n",
    "eclf2 = skVotingClassifier(estimators=[('lr', clf1), ('rf', clf2)]).fit(X, y)\n",
    "prob1 = eclf1.transform(X)\n",
    "prob2 = eclf2.transform(X)\n",
    "assert np.allclose(prob1, prob2)\n",
    "pred1 = eclf1.predict(X)\n",
    "pred2 = eclf2.predict(X)\n",
    "assert np.allclose(pred1, pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard voting\n",
    "X, y = load_iris(return_X_y = True)\n",
    "clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=15000, random_state=0)\n",
    "clf2 = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='soft').fit(X, y)\n",
    "eclf2 = skVotingClassifier(estimators=[('lr', clf1), ('rf', clf2)],\n",
    "                           voting='soft', flatten_transform=False).fit(X, y)\n",
    "prob1 = eclf1.transform(X)\n",
    "prob2 = eclf2.transform(X)\n",
    "assert np.allclose(prob1, prob2)\n",
    "prob1 = eclf1.predict_proba(X)\n",
    "prob2 = eclf2.predict_proba(X)\n",
    "assert np.allclose(prob1, prob2)\n",
    "pred1 = eclf1.predict(X)\n",
    "pred2 = eclf2.predict(X)\n",
    "assert np.array_equal(pred1, pred2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
